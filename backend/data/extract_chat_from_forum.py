import requests, time, re, os, csv, random
import pandas as pd
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import deque


# ===== é…ç½®åŒºåŸŸ =====

# --- åŸºç¡€è·¯å¾„è®¾ç½® ---
BASE_URL = "https://6do.world/t/topic/754330"  # è®ºå›å¸–å­ URLï¼Œå¯ä¿®æ”¹
BASE_DIR = r"G:\6doé¡¹ç›®"
INPUT_DIR = os.path.join(BASE_DIR, "CSVè¾“å…¥")       # CSV è¾“å‡ºç›®å½•
os.makedirs(INPUT_DIR, exist_ok=True)

# --- è¯·æ±‚ç›¸å…³è®¾ç½® ---
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                  "AppleWebKit/537.36 (KHTML, like Gecko) "
                  "Chrome/115.0.0.0 Safari/537.36"
}
TIMEOUT = 15                # å•æ¬¡è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
REQUEST_INTERVAL = 2.0      # æ­£å¸¸è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰
MAX_RETRIES = 3             # å•ä¸ªæ¥¼å±‚è¯·æ±‚æœ€å¤§é‡è¯•æ¬¡æ•°
MAX_WORKERS = 8             # å¹¶å‘çº¿ç¨‹æ•°ï¼ˆæŠ“å–æ¥¼å±‚æ—¶ä½¿ç”¨ï¼‰

# --- é™é€Ÿä¸é€€é¿ç­–ç•¥ ---
BACKOFF_BASE_DELAY = 5      # è§¦å‘ 429 æ—¶çš„åŸºå‡†é€€é¿æ—¶é—´ï¼ˆç§’ï¼‰
BACKOFF_MAX_DELAY = 60      # åŠ¨æ€é€€é¿æœ€å¤§æ—¶é—´ï¼ˆç§’ï¼‰
RETRY_EXTRA_DELAY = 10      # è¡¥æŠ“æ—¶çš„é¢å¤–å»¶è¿Ÿï¼ˆç§’ï¼‰ï¼Œé¿å…å’Œæ­£å¸¸æŠ“å–å†²çª

# --- è‡ªåŠ¨è¡¥æŠ“ ---
MAX_SUPPLEMENT_ROUNDS = 3   # è‡ªåŠ¨è¡¥æŠ“çš„æœ€å¤§è½®æ•°

# --- æ¥¼å±‚è‡ªåŠ¨æ¢æµ‹å‚æ•° ---
STAGE1_MAX = 1000           # Stage1 é¡ºåºæ¢æµ‹æœ€å¤§æ¥¼å±‚æ•°
STOP_ON_EMPTY = 5           # Stage1 è¿ç»­ç©ºé¡µæ•°é˜ˆå€¼
TAIL_MAX = 100              # Stage2 å°¾éƒ¨ç¡®è®¤æœ€å¤§æ£€æŸ¥é¡µæ•°
TAIL_STOP_EMPTY = 5         # Stage2 è¿ç»­ç©ºé¡µæ•°é˜ˆå€¼
PROGRESS_EVERY = 10         # æ¯ N é¡µè¾“å‡ºä¸€æ¬¡è¿›åº¦
MIN_ACCEPT = 10             # æ£€æµ‹ç»“æœå°äºæ­¤å€¼ â†’ æç¤ºäººå·¥ç¡®è®¤
MAX_ACCEPT = 2000           # æ£€æµ‹ç»“æœå¤§äºæ­¤å€¼ â†’ æç¤ºäººå·¥ç¡®è®¤

# ====================

def sanitize_filename(name: str) -> str:
    """æ¸…ç†æ–‡ä»¶åä¸­ä¸åˆæ³•æˆ–å¤šä½™å­—ç¬¦ï¼Œåªä¿ç•™ä¸­è‹±æ–‡ã€æ•°å­—å’Œéƒ¨åˆ†ç¬¦å·"""
    # å»é™¤ emoji ç­‰é BMP å­—ç¬¦
    name = re.sub(r"[\U00010000-\U0010ffff]", "", name)
    # å»æ‰ä¸éœ€è¦çš„ç¬¦å·
    name = re.sub(r"[\\/:*?\"<>|]", "", name)
    # å»æ‰å¤šä½™ç©ºæ ¼
    name = re.sub(r"\s+", "", name)
    # å¯é€‰ï¼šåªä¿ç•™ä¸­è‹±æ–‡ã€æ•°å­—ã€æ¨ªçº¿ã€ä¸‹åˆ’çº¿
    name = re.sub(r"[^0-9A-Za-z\u4e00-\u9fa5\-_]", "", name)
    return name

def simplify_title_for_filename(title: str) -> str:
    """
    ä»å¸–å­æ ‡é¢˜æå–ä¸»è¦éƒ¨åˆ†ï¼Œç”Ÿæˆæ ‡å‡†åŒ–æ–‡ä»¶å
    ä¾‹å¦‚ï¼š
    å…­åº¦ä¸–ç•ŒèŠå¤©åŒº202508 æ€»å¤‡ä»½ - ğŸ§—ğŸ»â€â™€ï¸èµ„æ·±ç½‘å‹è®¨è®ºåŒº - å…­åº¦ä¸–ç•Œ
    â†’ å…­åº¦ä¸–ç•ŒèŠå¤©åŒº202508
    """
    # å…ˆæ¸…ç† emoji å’Œç‰¹æ®Šç¬¦å·
    title = re.sub(r"[\U00010000-\U0010ffff]", "", title)
    title = re.sub(r"[\\/:*?\"<>|]", "", title)
    title = re.sub(r"\s+", " ", title).strip()

    # å¦‚æœæ ‡é¢˜é‡Œæœ‰â€œå…­åº¦ä¸–ç•ŒèŠå¤©åŒºâ€ï¼Œä¼˜å…ˆæå–å®ƒåŠåé¢çš„å¹´æœˆ
    m = re.search(r"(å…­åº¦ä¸–ç•ŒèŠå¤©åŒº\s*\d{6}(?:[-â€“]\d{6})?)", title)
    if m:
        return m.group(1).replace(" ", "")

    # å¦‚æœæ‰¾ä¸åˆ°ï¼Œåˆ™é€€åŒ–ä¸ºå‰20ä¸ªå­—ç¬¦
    return title[:20].replace(" ", "")


def fetch_page(url, session=None, is_retry=False):
    """
    æŠ“å–é¡µé¢ï¼Œæ”¯æŒé™æµåŠ¨æ€é€€é¿ + å¤šæ¬¡é‡è¯•
    """
    if session is None:
        session = requests.Session()
    delay = REQUEST_INTERVAL
    for attempt in range(1, MAX_RETRIES + 1):
        try:
            time.sleep(delay)
            response = session.get(url, timeout=TIMEOUT, headers=HEADERS)

            if response.status_code == 200:
                return response.text

            if response.status_code == 429:
                backoff_time = min(BACKOFF_BASE_DELAY * (2 ** (attempt - 1)), BACKOFF_MAX_DELAY)
                print(f"è¯·æ±‚å¤±è´¥({attempt}/{MAX_RETRIES}): {url}ï¼ŒåŸå› : 429 Too Many Requestsï¼Œé€€é¿ {backoff_time} ç§’")
                time.sleep(backoff_time)
                continue

            response.raise_for_status()
            return response.text

        except Exception as e:
            print(f"è¯·æ±‚å¤±è´¥({attempt}/{MAX_RETRIES}): {url}ï¼ŒåŸå› : {e}")
            backoff_time = min(BACKOFF_BASE_DELAY * (2 ** (attempt - 1)), BACKOFF_MAX_DELAY)
            time.sleep(backoff_time)

    if not is_retry:
        print(f"âš ï¸ {url} å¤šæ¬¡å¤±è´¥ï¼Œäº¤ç»™è¡¥æŠ“å¤„ç†")
    return None

def fetch_all_floors(base_url, max_floor):
    pending = deque(range(1, max_floor+1))
    results = {}

    while pending:
        floor = pending.popleft()
        url = f"{base_url}/{floor}"
        html = fetch_page(url)

        if html:
            results[floor] = parse_chat_transcripts(html)
        else:
            print(f"âŒ æ¥¼å±‚ {floor} æŠ“å–å¤±è´¥ï¼Œå°†ç¨åé‡è¯•")
            pending.append(floor)   # åŠ å›é˜Ÿåˆ—å°¾éƒ¨

        time.sleep(REQUEST_INTERVAL)  # æ§åˆ¶é—´éš”

    return results

def extract_post_title_and_yyyymm(html):
    """
    ä»å¸–å­æ ‡é¢˜æå–æ—¶é—´ä¿¡æ¯ï¼Œæ”¯æŒå¤šç§æ—¥æœŸæ ¼å¼ï¼š
    1. åŸºç¡€æ ¼å¼ï¼š202306, 2023-06, 2023å¹´6æœˆ, 2023.06
    2. èŒƒå›´æ ¼å¼ï¼š2023å¹´6æœˆè‡³8æœˆ, 2023.06-2023.08
    3. å­£åº¦æ ¼å¼ï¼š2023å¹´Q2, 2023å¹´ç¬¬äºŒå­£åº¦
    4. ä¸­æ–‡æœˆä»½ï¼š2023å¹´å…­æœˆ, 2023å¹´6æœˆ
    5. è·¨å¹´èŒƒå›´ï¼š2023å¹´12æœˆ-2024å¹´1æœˆ
    """
    soup = BeautifulSoup(html, "html.parser")
    title = soup.title.string if soup.title else ""
    # print(f"åŸå§‹æ ‡é¢˜: {title}")  # å¯å¼€å¯è°ƒè¯•
    
    yyyymm_list = []
    month_map = {
        'ä¸€æœˆ': '01', 'äºŒæœˆ': '02', 'ä¸‰æœˆ': '03', 'å››æœˆ': '04',
        'äº”æœˆ': '05', 'å…­æœˆ': '06', 'ä¸ƒæœˆ': '07', 'å…«æœˆ': '08',
        'ä¹æœˆ': '09', 'åæœˆ': '10', 'åä¸€æœˆ': '11', 'åäºŒæœˆ': '12',
        '1æœˆ': '01', '2æœˆ': '02', '3æœˆ': '03', '4æœˆ': '04',
        '5æœˆ': '05', '6æœˆ': '06', '7æœˆ': '07', '8æœˆ': '08',
        '9æœˆ': '09', '10æœˆ': '10', '11æœˆ': '11', '12æœˆ': '12'
    }
    
    # 1. åŒ¹é…è¿ç»­6ä½æ•°å­—æ ¼å¼ (202306)
    yyyymm_list += re.findall(r"(?<!\d)(\d{6})(?!\d)", title)
    
    # 2. åŒ¹é…å¸¦åˆ†éš”ç¬¦çš„å¹´æœˆ (2023-06, 2023.06, 2023/06, 2023å¹´06æœˆ)
    separators = r"[å¹´\-./]"
    pattern = fr"(?<!\d)(\d{{4}}){separators}(\d{{1,2}})(?:æœˆ)?(?!\d)"
    matches = re.findall(pattern, title)
    for year, month in matches:
        yyyymm_list.append(f"{year}{month.zfill(2)}")
    
    # 3. åŒ¹é…ä¸­æ–‡æœˆä»½ (2023å¹´å…­æœˆ)
    # ä¼˜åŒ–æ­£åˆ™ï¼Œä¸¥æ ¼åŒ¹é…æœˆä»½è¯
    cn_pattern = r"(?<!\d)(\d{4})å¹´(å?ä¸€?äºŒ?æœˆ|ä¸€æœˆ|äºŒæœˆ|ä¸‰æœˆ|å››æœˆ|äº”æœˆ|å…­æœˆ|ä¸ƒæœˆ|å…«æœˆ|ä¹æœˆ|åæœˆ|åä¸€æœˆ|åäºŒæœˆ)"
    cn_matches = re.findall(cn_pattern, title)
    for year, cn_month in cn_matches:
        if cn_month in month_map:
            yyyymm_list.append(f"{year}{month_map[cn_month]}")
    
    # 4. å¤„ç†æœˆä»½èŒƒå›´
    range_patterns = [
        fr"(\d{{4}}){separators}(\d{{1,2}})(?:æœˆ)?è‡³(\d{{1,2}})æœˆ",
        fr"(\d{{4}}){separators}(\d{{1,2}})(?:æœˆ)?-(\d{{1,2}})(?:æœˆ)?",
        fr"(\d{{4}}){separators}(\d{{1,2}})(?:æœˆ)?åˆ°(\d{{1,2}})(?:æœˆ)?",
        fr"(\d{{4}})(\d{{2}})-(\d{{4}})(\d{{2}})",
        fr"(\d{{4}}){separators}(\d{{1,2}})(?:æœˆ)?\s*[-~]\s*(\d{{4}}){separators}(\d{{1,2}})(?:æœˆ)?"
    ]
    for pattern in range_patterns:
        range_matches = re.findall(pattern, title)
        for match in range_matches:
            if len(match) == 3:
                year, start_month, end_month = match
                for m in range(int(start_month), int(end_month) + 1):
                    yyyymm_list.append(f"{year}{str(m).zfill(2)}")
            elif len(match) == 4:
                start_year, start_month, end_year, end_month = match
                start_date = pd.to_datetime(f"{start_year}{start_month.zfill(2)}", format="%Y%m")
                end_date = pd.to_datetime(f"{end_year}{end_month.zfill(2)}", format="%Y%m")
                current = start_date
                while current <= end_date:
                    yyyymm_list.append(current.strftime("%Y%m"))
                    current += pd.DateOffset(months=1)
    
    # 5. å¤„ç†å­£åº¦æ ¼å¼
    quarter_patterns = [
        r"(\d{4})å¹´[ç¬¬]?([ä¸€äºŒä¸‰å››1234])å­£åº¦",
        r"(\d{4})å¹´[Qq]([1234])"
    ]
    quarter_map = {'ä¸€': '1', 'äºŒ': '2', 'ä¸‰': '3', 'å››': '4'}
    quarters = {
        '1': ['01', '02', '03'],
        '2': ['04', '05', '06'],
        '3': ['07', '08', '09'],
        '4': ['10', '11', '12']
    }
    for pattern in quarter_patterns:
        q_matches = re.findall(pattern, title)
        for year, q in q_matches:
            quarter_num = quarter_map.get(q, q)
            for month in quarters.get(quarter_num, []):
                yyyymm_list.append(f"{year}{month}")
    
    # å»é‡å¹¶æ’åº
    yyyymm_list = sorted(list(set(yyyymm_list)))
    
    if not yyyymm_list:
        yyyymm = "æœªçŸ¥æ—¶é—´"
    elif len(yyyymm_list) == 1:
        yyyymm = yyyymm_list[0]
    else:
        # ç”¨æœˆä»½å·®åˆ¤æ–­è¿ç»­æ€§
        def month_diff(d1, d2):
            return (d2.year - d1.year) * 12 + (d2.month - d1.month)
        
        is_continuous = True
        dates = [pd.to_datetime(x, format="%Y%m") for x in yyyymm_list]
        for i in range(1, len(dates)):
            if month_diff(dates[i-1], dates[i]) > 1:
                is_continuous = False
                break
        
        if is_continuous:
            yyyymm = f"{yyyymm_list[0]}-{yyyymm_list[-1]}"
        else:
            yyyymm = f"{yyyymm_list[0]}_ç­‰å¤šä¸ªæœˆä»½"
    
    # print(f"æå–çš„æ—¶é—´ä¿¡æ¯: {yyyymm}")  # å¯å¼€å¯è°ƒè¯•
    return title, yyyymm

def parse_chat_transcripts(html):
    """è§£æèŠå¤©æ¶ˆæ¯ï¼ˆè‡ªåŠ¨æ¸…ç†ç”¨æˆ·åå‰ç¼€ï¼‰"""
    soup = BeautifulSoup(html, "html.parser")
    records = []

    for div in soup.find_all("div", class_="chat-transcript"):
        mid = div.get("data-message-id")
        username = div.get("data-username", "").strip()
        created_at = div.get("data-datetime")
        channel = div.get("data-channel-name", "æœªçŸ¥é¢‘é“")

        # å°è¯•è·å–çº¯æ¶ˆæ¯å†…å®¹
        # ä¼˜å…ˆæ‰¾ message åŒºå—ï¼Œå¦åˆ™å–æ•´ä¸ªæ–‡æœ¬
        msg_div = div.find("div", class_="chat-transcript-message")
        if msg_div:
            content = msg_div.get_text(separator="", strip=True)
        else:
            content = div.get_text(separator="", strip=True)

        # æ¸…ç†ä¸ç”¨æˆ·åé‡å¤çš„å‰ç¼€
        if username and content.startswith(username):
            content = content[len(username):].lstrip(" ï¼š: ")  # å»æ‰å…¨è§’/åŠè§’å†’å·ä¸ç©ºæ ¼

        # å»æ‰ç©ºæ¶ˆæ¯
        if not content:
            continue

        records.append({
            "message_id": mid,
            "username": username,
            "channel_name": channel,
            "content": content,
            "created_at": created_at
        })

    return records

def deduplicate_records(records):
    seen = set()
    unique = []
    for r in records:
        mid = r["message_id"]
        if mid and mid not in seen:
            seen.add(mid)
            unique.append(r)
    return unique

def get_max_floors(base_url):
    print("æ­£åœ¨è‡ªåŠ¨æ£€æµ‹æœ€å¤§æ¥¼å±‚æ•°ï¼ˆStage1ï¼‰...")
    seen_ids = set()
    last_floor_with_new_ids = 1
    floor = 1
    consecutive_empty = 0

    # Stage 1
    while True:
        test_url = base_url if floor == 1 else f"{base_url}/{floor}"
        html = fetch_page(test_url)
        if not html:
            print(f"æ¢æµ‹ä¸­æ–­ï¼šç¬¬ {floor} é¡µæ— æ³•è·å–æˆ–è¿”å›ç©ºå†…å®¹ï¼Œåœæ­¢ Stage1")
            break
        records = parse_chat_transcripts(html)
        ids = {r["message_id"] for r in records if r.get("message_id")}
        if not ids:
            consecutive_empty += 1
            if consecutive_empty >= STOP_ON_EMPTY:
                print(f"Stage1: è¿ç»­ {STOP_ON_EMPTY} é¡µæ— æœ‰æ•ˆæ¶ˆæ¯ï¼Œåœæ­¢ Stage1 æ¢æµ‹")
                break
        else:
            consecutive_empty = 0
            new_ids = ids - seen_ids
            if new_ids:
                last_floor_with_new_ids = floor
            seen_ids.update(ids)
        if floor % PROGRESS_EVERY == 0:
            print(f"Stage1 å·²æ¢æµ‹åˆ°ç¬¬ {floor} é¡µï¼Œå½“å‰ last_new_floor={last_floor_with_new_ids}")
        floor += 1
        if floor > STAGE1_MAX:
            print(f"è¾¾åˆ° Stage1 ä¸Šé™ {STAGE1_MAX}ï¼Œåœæ­¢ Stage1")
            break

    print(f"Stage1 å®Œæˆï¼Œè®°å½•åˆ°æœ€åå‡ºç°æ–°æ¶ˆæ¯çš„æ¥¼å±‚: {last_floor_with_new_ids}")

    # Stage 2
    print("å¼€å§‹å°¾éƒ¨ç¡®è®¤ï¼ˆStage2ï¼‰...")
    consecutive_no_new = 0
    check_floor = last_floor_with_new_ids + 1
    tail_checked = 0
    while tail_checked < TAIL_MAX:
        test_url = f"{base_url}/{check_floor}"
        html = fetch_page(test_url)
        if not html:
            consecutive_no_new += 1
            if consecutive_no_new >= TAIL_STOP_EMPTY:
                print(f"Stage2: è¿ç»­ {TAIL_STOP_EMPTY} é¡µæ— æ³•è·å–æˆ–æ— æ–°æ•°æ®ï¼Œåœæ­¢")
                break
        else:
            records = parse_chat_transcripts(html)
            ids = {r["message_id"] for r in records if r.get("message_id")}
            new_ids = ids - seen_ids if ids else set()
            if new_ids:
                last_floor_with_new_ids = check_floor
                seen_ids.update(ids)
                consecutive_no_new = 0
                print(f"Stage2: åœ¨ç¬¬ {check_floor} é¡µå‘ç°æ–°æ¶ˆæ¯ï¼Œæ›´æ–° last_floor={last_floor_with_new_ids}")
            else:
                consecutive_no_new += 1
                if consecutive_no_new >= TAIL_STOP_EMPTY:
                    print(f"Stage2: è¿ç»­ {TAIL_STOP_EMPTY} é¡µæ— æ–°æ•°æ®ï¼Œåœæ­¢")
                    break
        check_floor += 1
        tail_checked += 1

    print(f"Stage2 å®Œæˆï¼Œæœ€ç»ˆæ£€æµ‹åˆ°æœ€å¤§æ¥¼å±‚: {last_floor_with_new_ids}")

    if last_floor_with_new_ids < MIN_ACCEPT or last_floor_with_new_ids > MAX_ACCEPT:
        try:
            user_input = input(f"æ£€æµ‹ç»“æœå¯èƒ½å¼‚å¸¸ï¼ˆ{last_floor_with_new_ids}ï¼‰ï¼Œè¯·è¾“å…¥æ¥¼å±‚æ•°æˆ–å›è½¦æ¥å—è‡ªåŠ¨ç»“æœ: ").strip()
            if user_input:
                manual_val = int(user_input)
                print(f"ä½¿ç”¨äººå·¥è¾“å…¥æ¥¼å±‚æ•°: {manual_val}")
                return manual_val
        except Exception:
            pass

    return last_floor_with_new_ids

def crawl_post(base_url):
    print(f"å¼€å§‹æŠ“å–é¦–é¡µä»¥è·å–æ ‡é¢˜å’Œæ—¶é—´ä¿¡æ¯: {base_url}")
    first_page_html = fetch_page(base_url)
    if not first_page_html:
        print(f"[{base_url}] é¦–é¡µè¯·æ±‚å¤±è´¥ï¼Œè·³è¿‡")
        return

    soup = BeautifulSoup(first_page_html, "html.parser")
    title = soup.title.string if soup.title else "æœªå‘½å"

    # ç”Ÿæˆæ ‡å‡†æ ¼å¼æ–‡ä»¶å
    clean_title = simplify_title_for_filename(title)
    output_name = sanitize_filename(clean_title) + ".csv"
    output_file = os.path.join(INPUT_DIR, output_name)

    # è‡ªåŠ¨æ¢æµ‹æ¥¼å±‚
    max_floors = get_max_floors(base_url)

    # ç¬¬ä¸€æ¬¡æŠ“å–
    all_records = parse_chat_transcripts(first_page_html)
    fetched_floors = {1} if all_records else set()

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {executor.submit(fetch_and_parse_page, base_url, floor): floor
                   for floor in range(2, max_floors + 1)}
        for future in as_completed(futures):
            floor = futures[future]
            floor_records = future.result()
            if floor_records:
                fetched_floors.add(floor)
                all_records.extend(floor_records)

    # è‡ªåŠ¨è¡¥æŠ“ç¼ºå¤±æ¥¼å±‚
    missing_floors = set(range(1, max_floors + 1)) - fetched_floors
    round_num = 1
    while missing_floors and round_num <= MAX_SUPPLEMENT_ROUNDS:
        print(f"å¼€å§‹ç¬¬ {round_num} è½®è¡¥æŠ“ï¼Œç¼ºå¤±æ¥¼å±‚æ•°: {len(missing_floors)}")
        new_fetched = set()
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            futures = {executor.submit(fetch_and_parse_page, base_url, floor): floor
                       for floor in missing_floors}
            for future in as_completed(futures):
                floor = futures[future]
                floor_records = future.result()
                if floor_records:
                    new_fetched.add(floor)
                    all_records.extend(floor_records)
        missing_floors -= new_fetched
        print(f"ç¬¬ {round_num} è½®è¡¥æŠ“å®Œæˆï¼Œå‰©ä½™ç¼ºå¤±æ¥¼å±‚: {len(missing_floors)}")
        round_num += 1

    if missing_floors:
        print(f"âš ï¸ æœ€ç»ˆä»æœ‰ {len(missing_floors)} ä¸ªæ¥¼å±‚ç¼ºå¤±: {sorted(missing_floors)}")

    # å»é‡
    all_records = deduplicate_records(all_records)

    # è¾“å‡º
    with open(output_file, "w", newline="", encoding="utf-8-sig") as f:
        writer = csv.DictWriter(f, fieldnames=["message_id", "username", "channel_name", "content", "created_at"])
        writer.writeheader()
        writer.writerows(all_records)

    print(f"[{title}] æŠ“å–å®Œæˆï¼Œå…± {len(all_records)} æ¡æ¶ˆæ¯ï¼Œå·²ä¿å­˜åˆ° {output_file}")

def fetch_and_parse_page(base_url, floor, session=None, is_retry=False):
    """æŠ“å–å¹¶è§£æå•ä¸ªæ¥¼å±‚"""
    url = base_url if floor == 1 else f"{base_url}/{floor}"
    html = fetch_page(url, session=session, is_retry=is_retry)
    return parse_chat_transcripts(html) if html else []

if __name__ == "__main__":
    crawl_post(BASE_URL)
